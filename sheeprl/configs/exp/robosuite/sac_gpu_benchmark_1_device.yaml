# @package _global_
defaults:
  - override /algo: sac
  - override /env: robosuite
  - override /model_manager: sac
  - _self_

##############################################################
# PickPlace Benchmark                                        #
# Picking up the tomato sauce starting from a close position #
# All rewards enabled                                        #
##############################################################
exp_name: sac_Panda_PickPlace_gpu_benchmark_1_device
root_dir: sac/GPU_Benchmark

# Algorithm
algo:
  total_steps: 100000
  learning_starts: 5000
  per_rank_batch_size: 512
  replay_ratio: 1
  mlp_keys:
    encoder: [state, object-state]
# Checkpoint
checkpoint:
  every: 50000

# Buffer
buffer:
  size: 100000
  checkpoint: False
  sample_next_obs: False

env:
  num_envs: 1
  wrapper: 
    bddl_file: scenes/LIBERO_OBJECT_SCENE_pick_up_the_tomato_sauce_and_place_it_in_the_basket.bddl
    reward_shaping: True
    reward_stage_multipliers: [0.1, 0.35, 0.5, 0.7]
    use_camera_obs: True

fabric:
  accelerator: gpu
  precision: bf16-mixed
  devices: [0] # First bus 
  
metric:
  aggregator:
    metrics:
      Loss/value_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/alpha_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/entropy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}